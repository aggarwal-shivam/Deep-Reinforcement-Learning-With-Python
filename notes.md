# Important terms about the RL to recall later

- RL is like trial and error
- It is completely different from the supervised or unsupervised learning, because in both of these, model learns from the given data, but in RL model learns by interaction.
  
## Important Terms

- Agent
- Environment
- State and Action
- Reward
- Markov Decision Process
  - Markov chain or sequence: state, transition probability
  - Markov Reward Process: Markov chain + Reward function
  - Markov Decision Process: MRP + Actions
- Action Space: set of all actions
  - Continuous 
  - Discrete 
- Policy: defines the agent's behaviour in the environment
  - Deterministic policy: maps to a specific action
  - Stochastic policy: maps to a probability distribution
    - Categorical policy
    - Gaussian policy
- Episode: interaction between an agent and the environment
- Episodic and Continuous tasks
  - Episodic: a task with a terminal state
  - Continuous: a task with no terminal state
- Horizon: time steps till which agent will interact with the environment
  - Finite horizon
  - Infinite horizon
- Return: total rewards obtained by an agent during an episode
  - In case of episodic tasks, reward calculation is simple
  - In case of continuous task, how will we do this calculation?
    - Discount factor comes to rescue in this case
    - Discount factor ranges from 0 to 1. A high discount factor means, we will give high importance to the future rewards, while a low discount factor means that we will give more importance to the immediate rewards.
    - Discount factor 0 means: agent will only learn the immediate reward
    - Discount factor 1 means: agent will learn till infinity
- Value function
  - also known as the state value function, denotes the value of a state.
  - It tells what reward, the agent would obtain, starting from that state following some policy. 
  - Instead of directly taking the value of a state, we will take the expected return. It will help in case of stochastic policies.
  - So the value of a state is the expected return of the trajectory, starting from that state.
- Q function
  - Almost similar to the value function. The only difference is that here, we will calculate the return of a state-action pair, while in the calculation of the value function, we calculated the return of a state.
  - Similar to the value function, instead of using the direct value, we will use the expected value of the return.
  - Like the value function, Q function depends on the policy.
  - There can be many Q functions based on different policies. Optimal Q function is the one with the highest value, known as Q*
- Model based and Model Free Learning
  - Model based: The agent knows all the dynamics of the environment
  - Model free: The agent knows nothing about the environment
- Types of Environments
  - Deterministic and Stochastic environment
    - Deterministic: when we are sure that an action from a state s always lead to a state s'
    - Stochastic: When there is a randomness associated with the actions from a state
  - Discrete and Continuous environment
    - Discrete: When the action space of the env is discrete
    - Continuous: When the action space of the env is continuous
  - Episodic and Non-episodic env
    - Episodic: when agent's actions doesn't affect the future outcome
    - Non-episodic: When agent's actions affect the future outcome
  - Single and Multi-agent env
    - Depends on the number of agents in the environment.